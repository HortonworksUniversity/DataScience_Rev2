{
 "metadata": {
  "name": "",
  "signature": "sha256:a01a5be17dea52f6c6eca8af569ecdf72972e9b9087000d597a80db295c6ead4"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Method 1: Maximum Entropy POS"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Download the webtext corpus"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nltk.download('webtext')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Quick analysis of the amount of data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import webtext\n",
      "len(webtext.words())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##View the files in webtext"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for fileid in webtext.fileids() :\n",
      "    print fileid"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(webtext.raw('grail.txt'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##View the sentences from one of the files"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print webtext.raw('grail.txt')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sentences = webtext.sents('grail.txt')\n",
      "for sentence in sentences :\n",
      "    print sentence"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Tag a sentence"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nltk.pos_tag(sentences[3])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> The above command will fail if you do not have a POS tagger available.\n",
      "> Download `maxent_treebank_pos_tagger` (if necessary), which is the Maximum Entropy tagger"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nltk.download('maxent_treebank_pos_tagger');"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nltk.pos_tag(sentences[3])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Create a collection of tagged words in the first 100 sentences"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tagged_words = []\n",
      "for sentence in sentences[:100] :\n",
      "    tags = nltk.pos_tag(sentence)\n",
      "    for tag in tags :\n",
      "        tagged_words.append(tag)\n",
      "tagged_words"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Create a frequency distribution of the tags"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.probability import FreqDist \n",
      "fdist = FreqDist()\n",
      "for tag in tagged_words :\n",
      "    fdist[tag[1]] += 1\n",
      "fdist"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fdist.tabulate()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####There are 166 nouns, 95 proper nouns, 61 proper pronouns, and so on"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "-------------\n",
      "#Method 2: Decision Tree POS\n",
      "##Create a Decision Tree based on the most common suffixes"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Step 1: Find the most common suffixes"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.probability import FreqDist \n",
      "fdist2 = FreqDist()\n",
      "for sentence in sentences :\n",
      "    for word in sentence :\n",
      "        word = word.lower()\n",
      "        fdist2[word[-1:]] += 1\n",
      "        fdist2[word[-2:]] += 1\n",
      "        fdist2[word[-3:]] += 1\n",
      "fdist2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Display the 100 most frequent suffixes"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "top_100 = fdist2.keys()[:100]\n",
      "top_100"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Step 2: Define a features function "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def pos_features(word) :\n",
      "    features = {}\n",
      "    for suffix in top_100 :\n",
      "        features['endswith(%s)' % suffix] = word.lower().endswith(suffix)\n",
      "    return features"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Step 3: Extract the features from the 'Holy Grail'"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "featuresets = [(pos_features(n),g) for (n,g) in tagged_words]\n",
      "featuresets"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Step 4: We will use half the data as training data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(featuresets)\n",
      "train_size = int(len(featuresets) * 0.5)\n",
      "print train_size"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Step 5: Train a Decision Tree with our training data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "classifier = nltk.DecisionTreeClassifier.train(featuresets[:train_size])\n",
      "classifier"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Step 6: Let's see how accurate our classifier is"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nltk.classify.accuracy(classifier, featuresets[train_size:])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Step 7: Classify some new data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "classifier.classify(pos_features('knights'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "classifier.classify(pos_features('wound'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "classifier.classify(pos_features('banging'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Step 8: View the decision tree"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print classifier.pseudocode(depth=10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---------\n",
      "#Named Entity Recognition"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ne = nltk.ne_chunk(tagged_words)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nltk.download('maxent_ne_chunker')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ne = nltk.ne_chunk(tagged_words)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nltk.download('words')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ne = nltk.ne_chunk(tagged_words, binary=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print ne"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ne2 = nltk.ne_chunk(tagged_words)\n",
      "print ne2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}

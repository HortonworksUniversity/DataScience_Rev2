{
 "metadata": {
  "name": "",
  "signature": "sha256:ae11133d07a4b6820b442f087e4b34ba79529d8790f5429e37761783d1a11776"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Method 1: Maximum Entropy POS"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Download the webtext corpus"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nltk.download('webtext')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[nltk_data] Downloading package 'webtext' to /root/nltk_data...\n",
        "[nltk_data]   Package webtext is already up-to-date!\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "True"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Quick analysis of the amount of data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import webtext\n",
      "len(webtext.words())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "396736"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##View the files in webtext"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for fileid in webtext.fileids() :\n",
      "    print fileid"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "firefox.txt\n",
        "grail.txt\n",
        "overheard.txt\n",
        "pirates.txt\n",
        "singles.txt\n",
        "wine.txt\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(webtext.raw('grail.txt'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "65003"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##View the sentences from one of the files"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print webtext.raw('grail.txt')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
              ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sentences = webtext.sents('grail.txt')\n",
      "for sentence in sentences :\n",
      "    print sentence"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [

       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Tag a sentence"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nltk.pos_tag(sentences[3])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> The above command will fail if you do not have a POS tagger available.\n",
      "> Download `maxent_treebank_pos_tagger` (if necessary), which is the Maximum Entropy tagger"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nltk.download('maxent_treebank_pos_tagger');"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[nltk_data] Downloading package 'maxent_treebank_pos_tagger' to\n",
        "[nltk_data]     /root/nltk_data...\n",
        "[nltk_data]   Package maxent_treebank_pos_tagger is already up-to-\n",
        "[nltk_data]       date!\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nltk.pos_tag(sentences[3])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "[('ARTHUR', 'NNP'),\n",
        " (':', ':'),\n",
        " ('It', 'PRP'),\n",
        " ('is', 'VBZ'),\n",
        " ('I', 'PRP'),\n",
        " (',', ','),\n",
        " ('Arthur', 'NNP'),\n",
        " (',', ','),\n",
        " ('son', 'NN'),\n",
        " ('of', 'IN'),\n",
        " ('Uther', 'NNP'),\n",
        " ('Pendragon', 'NNP'),\n",
        " (',', ','),\n",
        " ('from', 'IN'),\n",
        " ('the', 'DT'),\n",
        " ('castle', 'NN'),\n",
        " ('of', 'IN'),\n",
        " ('Camelot', 'NNP'),\n",
        " ('.', '.')]"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Create a collection of all the tagged words"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tagged_words = []\n",
      "for sentence in sentences :\n",
      "    tags = nltk.pos_tag(sentence)\n",
      "    for tag in tags :\n",
      "        tagged_words.append(tag)\n",
      "tagged_words"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
]"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Create a frequency distribution of the tags"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.probability import FreqDist \n",
      "fdist = FreqDist()\n",
      "for tag in tagged_words :\n",
      "    fdist.inc(tag[1])\n",
      "fdist"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "<FreqDist with 40 samples and 16978 outcomes>"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fdist.tabulate()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [      ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####There are 2,417 proper nouns, 2,182 nouns, 1,060 proper pronouns, and so on"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "-------------\n",
      "#Method 2: Decision Tree POS\n",
      "##Create a Decision Tree based on the most common suffixes"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Step 1: Find the most common suffixes"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.probability import FreqDist \n",
      "fdist2 = FreqDist()\n",
      "for sentence in sentences :\n",
      "    for word in sentence :\n",
      "        word = word.lower()\n",
      "        fdist2.inc(word[-1:])\n",
      "        fdist2.inc(word[-2:])\n",
      "        fdist2.inc(word[-3:])\n",
      "fdist2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 14,
       "text": [
        "<FreqDist with 956 samples and 50934 outcomes>"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Display the 100 most frequent suffixes"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "top_100 = fdist2.keys()[:100]\n",
      "top_100"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 15,
       "text": [
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Step 2: Define a features function "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def pos_features(word) :\n",
      "    features = {}\n",
      "    for suffix in top_100 :\n",
      "        features['endswith(%s)' % suffix] = word.lower().endswith(suffix)\n",
      "    return features"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Step 3: Extract the features from the 'Holy Grail'"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "featuresets = [(pos_features(n),g) for (n,g) in tagged_words]\n",
      "featuresets"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 17,
       "text": []"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Step 4: We will use half the data as training data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(featuresets)\n",
      "train_size = int(len(featuresets) * 0.5)\n",
      "print train_size"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "16978\n",
        "8489\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Step 5: Train a Decision Tree with our training data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "classifier = nltk.DecisionTreeClassifier.train(featuresets[:train_size])\n",
      "classifier"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 19,
       "text": [
        "<nltk.classify.decisiontree.DecisionTreeClassifier at 0x7fd417b14990>"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Step 6: Let's see how accurate our classifier is"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nltk.classify.accuracy(classifier, featuresets[train_size:])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 20,
       "text": [
        "0.6557898456826481"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Step 7: Classify some new data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "classifier.classify(pos_features('knights'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 21,
       "text": [
        "'NNS'"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "classifier.classify(pos_features('wound'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 22,
       "text": [
        "'NN'"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "classifier.classify(pos_features('banging'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 23,
       "text": [
        "'VBG'"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Step 8: View the decision tree"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print classifier.pseudocode(depth=10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---------\n",
      "#Named Entity Recognition"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ne = nltk.ne_chunk(tagged_words)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "LookupError",
       "evalue": "\n**********************************************************************\n  Resource\n  'chunkers/maxent_ne_chunker/english_ace_multiclass.pickle' not\n  found.  Please use the NLTK Downloader to obtain the resource:\n  >>> nltk.download()\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************",
       "output_type": "pyerr",
       "traceback": [  ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nltk.download('maxent_ne_chunker')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[nltk_data] Downloading package 'maxent_ne_chunker' to\n",
        "[nltk_data]     /root/nltk_data...\n",
        "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 26,
       "text": [
        "True"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ne = nltk.ne_chunk(tagged_words)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "LookupError",
       "evalue": "\n**********************************************************************\n  Resource 'corpora/words' not found.  Please use the NLTK\n  Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************",
       "output_type": "pyerr",
       "traceback": [  ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nltk.download('words')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[nltk_data] Downloading package 'words' to /root/nltk_data...\n",
        "[nltk_data]   Unzipping corpora/words.zip."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 28,
       "text": [
        "True"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ne = nltk.ne_chunk(tagged_words, binary=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print ne"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
       
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ne2 = nltk.ne_chunk(tagged_words)\n",
      "print ne2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
              ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}